---
layout: presentation.njk
title: "Rapid Prototyping of a Positioning System"
presentation_title: "<span style='font-size: 100%'>Rapid Prototyping of a Positioning System</span>"
subtitle: "Using the OpenHPS Framework"
author: Maxim Van de Wynckel
logo: true
header_logo: /images/misc/openhps-logo.svg
affiliation: Web & Information Systems Engineering Lab</br>Vrije Universiteit Brussel
width: 1280
height: 720
hide_slide_numbers: true
---
{% decktape title, page %}

<style>
.reveal pre {
    font-size: .65em;
}
.reveal small {
    font-size: .9em;
}
</style>

<section>
    <section data-markdown data-auto-animate data-timing="20">
        {% markdown %}
            ## Positioning System
            > *"A positioning system is a mechanism for determining the position of an object in space."*\
            > [<small>*- Wikipedia (2022)*</small>](https://en.wikipedia.org/wiki/Positioning_system)
        {% endmarkdown %}
        <aside class="notes">
        Starting off with the basics, a positioning system is a defined as a mechanism for
        deterimining the position of an object in space.
        </aside>
    </section>
    <section data-markdown data-auto-animate data-timing="20">
    {% markdown %}
        ## Positioning System
        > *"A positioning system is a mechanism for determining the position of an **object** in **space**."*\
        > [<small>*- Wikipedia (2022)*</small>](https://en.wikipedia.org/wiki/Positioning_system)

        **Object**\
        What are you tracking? A person, an asset or a phone?

        **Space**\
        Outdoor, indoor, under water or on a table?
    {% endmarkdown %}
    <aside class="notes">
        The 'object' and 'space' in this definition are open for interpretation. Whether you want to track a person
        an asset or a device beloning to a person.

        Similarly, the space can be outdoor - like we are used with GPS, but it can also be indoor or even
        on a smaller scale such as a table or physical game board.
    </aside>
</section>

</section>

<section data-markdown data-auto-animate data-timing="20">
    {% markdown %}
        ## Use Cases
        - **Navigation**\
            *Navigate a person from point A to point B*
        - **Tracking**\
            *Asset tracking, customer tracking, tracking items on a table*
        - **Location Awareness**\
            *Trigger an action whenever a specific person is in a room*
        - **Mapping**\
            *Geospatial mapping of an environment*
    {% endmarkdown %}
    <aside class="notes">
    Use cases usually involve around four main goals. The navigation of a person from one point to another,
    the tracking of customers, employees or assets. Location awareness to simply have knowledge on the rough
    position or trajectory of a person. And finally, a positioning system can be used as a tool for mapping geospatial
    environments.
    </aside>
</section>

<section>
    <section data-markdown data-auto-animate data-timing="20">
        {% markdown %}
            ## Technologies
            *Technologies used to obtain sensor data for positioning*
            - Camera (stereoscopic, monocular, omnidirectional)
            - Beacons (ultrawideband, Bluetooth, ultrasound)
            - LIDAR
            - Inertial measurement unit (IMU)
            - Visible light communication
            - ...
        {% endmarkdown %}
        <img src="images/technologies/overview.png" style="position: absolute; right: 0; top: 160px; width: 40%;">
        <aside class="notes">
        Outdoors we rely on GPS signals, but depending on the use case there are plenty of technologies to choose from.
        Different types of cameras can be used to obtain sensor data, Bluetooth beacons
        were one of the very early go-to technologies for indoor positioning. You have wifi access points, the newer
        Ultrawideband beacons that start to be supported in modern smartphones and many more
        </aside>
    </section>
</section>


<section>
    <section data-markdown data-auto-animate data-timing="20">
        {% markdown %}
            ## Algorithms
            *Algorithms used to process sensor data*
            - Lateration
            - Proximity positioning
            - Signal propagation
            - Fingerprinting
            - Computer vision
            - Dead reckoning
            - Sensor fusion
            - ...
        {% endmarkdown %}
        <img src="images/Word Art.png" style="position: absolute; right: 0; top: 150px; width: 50%;">
        <aside class="notes">
        Likewise, for each of these technologies - different algorithms can be chosen to process the
        sensor data that you obtain from these technologies, again each with their own pro's and con's.
        Proximity positioning can for example be very reliable, but is not accurate as it takes the position
        of the item that is within closest proximity. Choosing the combination of technologies and algorithms
        can be a challenge that often requires experimenting and tweaking to fit the goal that you want to achieve.
        What is more, the public available algorithms do not often follow a same data structure making the implementation
        of a novel positioning system that is optimized for the task a difficult journey.
        </aside>
    </section>
</section>

<section data-markdown data-auto-animate data-timing="20">
    {% markdown %}
        ## Open Source Solutions
        - AnyPlace *[https://anyplace.cs.ucy.ac.cy/](https://anyplace.cs.ucy.ac.cy/)*
        - FIND *[https://github.com/schollz/find3](https://github.com/schollz/find3)*
        - IndoorLocation *[https://github.com/IndoorLocation](https://github.com/IndoorLocation)*
        - Navigine *[https://github.com/Navigine](https://github.com/Navigine)*
        - RedPin *[http://redpin.org/](http://redpin.org/)*
        - Traccar *[https://github.com/traccar](https://github.com/traccar)*
        - TraceMeNow *[https://isislab-unisa.github.io/trace-me-now](https://isislab-unisa.github.io/trace-me-now)*
    {% endmarkdown %}
    <aside class="notes">
    Various open source solutions exist for creating a positioning system such as AnyPlace, Find or IndoorLocation. but
    all of these solutions focus on a specific use case or do not offer you the flexibility to choose the positioning algorithms.

    For developers who want to implement the use case that the open source solution is based on, this is a positive thing.
    As they do not need to experiment or test new things - but for developers who want to create new prototypes
    this is a problem.
    </aside>
</section>

<section>
    <section data-markdown data-auto-animate data-timing="20">
        {% markdown %}
            ## OpenHPS
            ### An Open Source Hybrid Positioning System
        {% endmarkdown %}
        <img width="61%" class="shadow" style="position: absolute; left: 0;" src="../ipin2021/images/openhps-site.png"/>
        <img width="260px" style="position: absolute; right: 100px; top: 155px;" src="images/badge_license.svg"/>
        <img width="260px" style="position: absolute; right: 100px; top: 215px;" src="images/badge_typescript.svg"/>
        <img width="260px" style="position: absolute; right: 100px; top: 275px;" src="images/badge_website.svg"/>
        <aside class="notes">
        This is where OpenHPS comes into place. OpenHPS is an open source hybrid positioning system developed in Typescript
        that can run both on a server, smartphone, embedded device and even the browser.
        </aside>
    </section>
    <section data-markdown data-auto-animate data-timing="40">
        {% markdown %}
            ## OpenHPS
            ### An Open Source Hybrid Positioning System
            - Any technology
            - Any algorithm
            - Various use cases
            - Flexibile processing and output
                - Prefer accuracy over battery consumption, reliability, ...
            - Aimed towards developers and researchers
        {% endmarkdown %}
        <aside class="notes">
        With OpenHPS we aim to offer a framework for developers and researchers that
        can support a variety of positioning technologies, algorithms and uses cases. We also aim to offer a lot of flexibility in the outcome of the position,
        weither it is a developer wishing to offer the most accurate position, power optimized solution or
        simply a reliable system that can rougly (but reliably) indicate weither or not you are in a specific room.
        </aside>
    </section>
    <section data-markdown data-auto-animate data-timing="25">
        <h2 data-id="title-processnetworkdesign">Process Network Design</h2>
        <img width="90%" src="images/architecture.svg"/>
        <aside class="notes">
        When creating a positioning system, you obtain sensor data from multiple different sensors. You process the data until at some stage in your processing
        you merge the sensor data to obtain a position that you can show to your user.
        </aside>
    </section>
    <section data-markdown data-auto-animate data-timing="30">
        <h2 data-id="title-processnetworkdesign">Process Network Design ...</h2>
        <img height="30%" src="images/architecture.svg"/>
        <img height="45%" data-id="architecture-graph" src="images/architecture2.svg"/>
        <aside class="notes">
        OpenHPS focuses on this method of developing a positioning system by allowing the creation of a process network with graph topology. 
        We identify source nodes that provide data, processing nodes that process this data and finally sink nodes that store or display this data. 
        We call this complete process network from source to sink a positioning model. 
        </aside>
    </section>
    <section data-markdown data-auto-animate data-timing="30">
        <h2 data-id="title-processnetworkdesign">Process Network Design ...</h2>
        <img height="30%" src="images/architecture.svg"/>
        <img height="45%" data-id="architecture-graph" src="images/architecture2_source.svg"/>
    </section>
    <section data-markdown data-auto-animate data-timing="30">
        <h2 data-id="title-processnetworkdesign">Process Network Design ...</h2>
        <img height="30%" src="images/architecture.svg"/>
        <img height="45%" data-id="architecture-graph" src="images/architecture2_processing.svg"/>
    </section>
    <section data-markdown data-auto-animate data-timing="30">
        <h2 data-id="title-processnetworkdesign">Process Network Design ...</h2>
        <img height="30%" src="images/architecture.svg"/>
        <img height="45%" data-id="architecture-graph" src="images/architecture2_sink.svg"/>
    </section>
    <section data-markdown data-auto-animate data-timing="35">
        <h2 data-id="title-processnetworkdesign">Process Network Design ...</h2>
        <img height="70%" style="margin-top: 1em" data-id="architecture-graph" src="images/architecture3.svg"/>
        <aside class="notes">
        On top of this, every node in our process network has access to a range of 'Services' that either manage the persistent storage of data that is being handled or
        handle background processing of information that is already pushed through the network. Each service can be added to multiple positioning models, allowing data
        exchange between the calibration part of a process network and the actual online stage used on a production ready system.
        </aside>
    </section>
    <section data-markdown data-auto-animate data-timing="40">
        <h2 data-id="title-modularity">Modularity</h2>
        <img width="100%" data-id="stack" style="margin-top: 2em" src="../ipin2021/images/stack.svg"/>
        <aside class="notes">
        By creating this process network design with shared services we allow for a very modular ecosystem where individual nodes or services
        can be reused throughout multiple different packages. Our core component offers the main concepts
        needed to design a process network for processing sensor data to an output position while other components
        handle the communication, data storage or add additional algorithms.
        </aside>
    </section>
</section>

<section>
    <section data-markdown data-auto-animate data-timing="20">
        <h2 data-id="title-modularity">Modularity ...</h2>
        <img width="90%" style="margin-top: -0.3em" data-id="stack-examples" src="images/stack_examples.svg"/>
    </section>
    <section data-markdown data-auto-animate data-timing="20">
        <h2 data-id="title-modularity">Modularity ...</h2>
        <img width="90%" style="margin-top: -0.3em" data-id="stack-examples" src="images/stack_examples_1.svg"/>
    </section>
    <section data-markdown data-auto-animate data-timing="20">
        <h2 data-id="title-modularity">Modularity ...</h2>
        <img width="90%" style="margin-top: -0.3em" data-id="stack-examples" src="images/stack_examples_2.svg"/>
    </section>
    <section data-markdown data-auto-animate data-timing="20">
        <h2 data-id="title-modularity">Modularity ...</h2>
        <img width="90%" style="margin-top: -0.3em" data-id="stack-examples" src="images/stack_examples_3.svg"/>
    </section>
    <section data-markdown data-auto-animate data-timing="20">
        <h2 data-id="title-modularity">Modularity ...</h2>
        <img width="90%" style="margin-top: -0.3em" data-id="stack-examples" src="images/stack_examples_4.svg"/>
    </section>
</section>

<section>
    <section data-markdown data-auto-animate data-timing="30">
        {% markdown %}
            ## Data Processing
        {% endmarkdown %}
        <img height="80%" src="images/dataprocessing.svg" class="logo">
        <aside class="notes">
        The data that we process in our process network is an important aspect of the core of OpenHPS.
        We have knowledge, raw data and processed data. Knowledge is everything that we know - such as sensors, radio transmitters
        or building layouts. Raw data is unprocessed data that we want to process, such as individual video or image frames, unfiltered
        accelerometer data. And finally, we have the processed data that will eventually become knowledge.
        </aside>
    </section>
    <section data-markdown data-auto-animate data-timing="40">
        {% markdown %}
            ## DataObject
            
        {% endmarkdown %}
        <div class="row">
            <div class="col-4">
                <img width="100%" style="margin-top: 1em" src="images/dataobject.svg">
            </div>
            <div class="col-8">
            {% markdown %}
                ```ts
                // Data object for the person we are tracking
                const me = new DataObject("mvdewync@vub.be");
                me.displayName = "Maxim Van de Wynckel";

                // Phone belonging to the person
                const phone = new DataObject()
                phone.displayName = "Maxim's Phone";
                phone.setParent(me);

                // Watch belonging to the person
                const watch = new DataObject();
                watch.displayName = "Maxim's Android Watch";
                watch.setParent(me);

                // IP camera identified by MAC
                const camera = new CameraObject("80:bb:7c:37:0e:02");
                camera.width = 1980;
                camera.height = 1024;
                camera.fps = 30;
                camera.setPosition(/* ... */);
                ```
            {% endmarkdown %}
            </div>
        </div>
    </section>
    <section data-markdown data-auto-animate data-timing="40">
        {% markdown %}
            ## DataObject
            
        {% endmarkdown %}
        <div class="row">
            <div class="col-4">
                <img width="100%" style="margin-top: 1em" src="images/dataobject.svg">
            </div>
            <div class="col-8">
            {% markdown %}
                ```ts {0-2}
                // Data object for the person we are tracking
                const me = new DataObject("mvdewync@vub.be");
                me.displayName = "Maxim Van de Wynckel";

                // Phone belonging to the person
                const phone = new DataObject();
                phone.displayName = "Maxim's Phone";
                phone.setParent(me);

                // Watch belonging to the person
                const watch = new DataObject();
                watch.displayName = "Maxim's Android Watch";
                watch.setParent(me);

                // IP camera identified by MAC
                const camera = new CameraObject("80:bb:7c:37:0e:02");
                camera.width = 1980;
                camera.height = 1024;
                camera.fps = 30;
                camera.setPosition(/* ... */);
                ```
            {% endmarkdown %}
            </div>
        </div>
    </section>
    <section data-markdown data-auto-animate data-timing="40">
        {% markdown %}
            ## DataObject
            
        {% endmarkdown %}
        <div class="row">
            <div class="col-4">
                <img width="100%" style="margin-top: 1em" src="images/dataobject.svg">
            </div>
            <div class="col-8">
            {% markdown %}
                ```ts {4-12}
                // Data object for the person we are tracking
                const me = new DataObject("mvdewync@vub.be");
                me.displayName = "Maxim Van de Wynckel";

                // Phone belonging to the person
                const phone = new DataObject();
                phone.displayName = "Maxim's Phone";
                phone.setParent(me);

                // Watch belonging to the person
                const watch = new DataObject();
                watch.displayName = "Maxim's Android Watch";
                watch.setParent(me);

                // IP camera identified by MAC
                const camera = new CameraObject("80:bb:7c:37:0e:02");
                camera.width = 1980;
                camera.height = 1024;
                camera.fps = 30;
                camera.setPosition(/* ... */);
                ```
            {% endmarkdown %}
            </div>
        </div>
    </section>
    <section data-markdown data-auto-animate data-timing="40">
        {% markdown %}
            ## DataObject
            
        {% endmarkdown %}
        <div class="row">
            <div class="col-4">
                <img width="100%" style="margin-top: 1em" src="images/dataobject.svg">
            </div>
            <div class="col-8">
            {% markdown %}
                ```ts {14-19}
                // Data object for the person we are tracking
                const me = new DataObject("mvdewync@vub.be");
                me.displayName = "Maxim Van de Wynckel";

                // Phone belonging to the person
                const phone = new DataObject();
                phone.displayName = "Maxim's Phone";
                phone.setParent(me);

                // Watch belonging to the person
                const watch = new DataObject();
                watch.displayName = "Maxim's Android Watch";
                watch.setParent(me);

                // IP camera identified by MAC
                const camera = new CameraObject("80:bb:7c:37:0e:02");
                camera.width = 1980;
                camera.height = 1024;
                camera.fps = 30;
                camera.setPosition(/* ... */);
                ```
            {% endmarkdown %}
            </div>
        </div>
    </section>
    <section data-markdown data-auto-animate data-timing="30">
        {% markdown %}
            ## Absolute and Relative Positions
        {% endmarkdown %}
        <div class="row">
            <div class="col-5">
                {% markdown %}
                    **Absolute**
                    - 2D, 3D, geographical, ...
                    - Within a reference space

                    **Relative**
                    - Distance, angle, velocity, ...
                    - Relative to another *object*
                {% endmarkdown %}
            </div>
            <div class="col-7">
            {% markdown %}
                ```ts
                // Absolute geographical position
                me.setPosition(new GeographicalPosition(
                    50.8204, 4.3921
                ));

                // Relative position(s) to another object
                me.addRelativePosition(new RelativeDistance(
                    "9F:F1:90:4C:F5:6A", 5.2, LengthUnit.METER
                ));
                me.addRelativePosition(new RelativeDistance(
                    "DC:0F:14:B2:6B:80", 1.4, LengthUnit.METER
                ));
                ```
            {% endmarkdown %}
            </div>
        </div>
        <img src="images/position.svg" width="50%">
    </section>
</section>

<section>
   <section data-markdown data-auto-animate data-timing="80">
        <h2 data-id="title-dataframe">DataFrame</h2>
        <div class="row">
            <div class="col-4">
                <img width="100%" src="images/dataframe.svg"/>
            </div>
            <div class="col-8">
            {% markdown %}
                ```ts
                // Sensor that captured the frame
                const camera = new CameraObject();

                // Create a new frame
                const frame = new VideoFrame();
                frame.source = camera;
                frame.image = myImage;

                // Add detected objects to frame
                frame.addObject(/* ... */);
                frame.addObject(/* ... */);
                frame.addObject(/* ... */);
                ```
            {% endmarkdown %}
            </div>
        </div>
        <img width="50%" src="images/dataframe2.svg"/>
    </section>
    <section data-markdown data-auto-animate data-timing="0">
        <h2 data-id="title-dataframe">DataFrame ...</h2>
        {% markdown %}
            ### Pushing Data

        {% endmarkdown %}
        <img width="100%" src="../ipin2021/images/nodes-push.svg"/>
    </section>
    <section data-markdown data-auto-animate data-timing="0">
        <h2 data-id="title-dataframe">DataFrame ...</h2>
        {% markdown %}
            ### Pulling Data

        {% endmarkdown %}
        <img width="100%" src="../ipin2021/images/nodes-pull.svg"/>
    </section>
</section>

<section>
    <section data-markdown data-auto-animate data-timing="0">
        {% markdown %}
            ## Positioning Model
        {% endmarkdown %}
        <div class="row">
            <div class="col-12">
            {% markdown %}
                ```ts
                ModelBuilder.create()
                    .from(new CallbackSourceNode(() => {
                        const myObject = new DataObject("mvdewync");
                        const frame = new DataFrame();
                        frame.addObject(myObject);
                        return frame;
                    }))
                    .via(new CallbackNode((frame: DataFrame) => { /* ... */ }))
                    .to(new CallbackSinkNode((frame: DataFrame) => { /* ... */ }))
                    .build().then((model: Model) => { /* ... */ });
                ```
            {% endmarkdown %}
            </div>
        </div>
        <img src="images/positioningmodel.svg" width="70%">
    </section>
</section>

<section>
    <section data-markdown data-auto-animate data-timing="20">
        {% markdown %}
            ## Example
            <div class="row" style="margin-top: 100px;">
                <div class="col-6">
                    <img height="100%" src="images/sphero.jpg">
                </div>
                <div class="col-6">
                    <img height="100%" src="images/demo-overview.png">
                </div>
            </div>
        {% endmarkdown %}
    </section>
    <section data-markdown data-auto-animate data-timing="20">
        {% markdown %}
            ## Example ...
            <img data-id="model" height="80%" src="images/model2.svg">
        {% endmarkdown %}
    </section>
    <section data-markdown data-auto-animate data-timing="20">
        {% markdown %}
            ## Example ...
            <img data-id="model" height="80%" src="images/model_sources.svg">
        {% endmarkdown %}
    </section>
    <section data-markdown data-auto-animate data-timing="20">
        {% markdown %}
            ## Example ...
            <img data-id="model" height="80%" src="images/model_sources_1.svg">
        {% endmarkdown %}
    </section>
    <section data-markdown data-auto-animate data-timing="20">
        {% markdown %}
            ## Example ...
            <img data-id="model" height="80%" src="images/model_sources_2.svg">
        {% endmarkdown %}
    </section>
    <section data-markdown data-auto-animate data-timing="20">
        {% markdown %}
            ## Example ...
            <img data-id="model" height="80%" src="images/model_sources_3.svg">
        {% endmarkdown %}
    </section>
    <section data-markdown data-auto-animate data-timing="20">
        {% markdown %}
            ## Example ...
            <img data-id="model" height="80%" src="images/model_branch.svg">
        {% endmarkdown %}
    </section>
    <section data-markdown data-auto-animate data-timing="20">
        {% markdown %}
            ## Example ...
            <img height="80%" src="images/image-transform.svg">
        {% endmarkdown %}
    </section>
    <section data-markdown data-auto-animate data-timing="20">
        <style>
        .reveal pre code {
            max-height: 555px;
        }
        </style>
        {% markdown %}
            ## Example ...
        {% endmarkdown %}
        <div class="row">
            <div class="col-1">
                <img data-id="model-branch" src="images/model_branch_2.svg" width="100%"/>
            </div>
            <div class="col-11">
                {% markdown %}
                ```ts
                GraphBuilder.create()
                    .from(new VideoSource({
                        autoPlay: true,
                        fps: 30,
                        throttleRead: true,
                        source: new CameraObject("sphero_video")
                    }).load("/dev/video2"))
                    .via(new ImageTransformNode({
                        src: [
                            new OpenCV.Point2(307, 120),
                            new OpenCV.Point2(1473, 87),
                            new OpenCV.Point2(1899, 891),
                            new OpenCV.Point2(20, 1024),
                        ],
                        height: 800,
                        width: 1040
                    }))
                    .via(new ColorMaskProcessing({
                        minRange: [90, 50, 50],
                        maxRange: [140, 255, 255]
                    }))
                    .via(new ContourDetectionNode()) // Custom
                    .to();
                ```
                {% endmarkdown %}
            </div>
        </div>
    </section>
    <section data-markdown data-auto-animate data-timing="20">

        {% markdown %}
            ## Example ...
        {% endmarkdown %}
        <div class="row">
            <div class="col-1">
                <img data-id="model-branch" src="images/model_branch_3.svg" width="100%"/>
            </div>
            <div class="col-11">
                {% markdown %}
                ```ts {1-6}
                GraphBuilder.create()
                    .from(new VideoSource({
                        autoPlay: true,
                        fps: 30,
                        throttleRead: true,
                        source: new CameraObject("sphero_video")
                    }).load("/dev/video2"))
                    .via(new ImageTransformNode({
                        src: [
                            new OpenCV.Point2(307, 120),
                            new OpenCV.Point2(1473, 87),
                            new OpenCV.Point2(1899, 891),
                            new OpenCV.Point2(20, 1024),
                        ],
                        height: 800,
                        width: 1040
                    }))
                    .via(new ColorMaskProcessing({
                        minRange: [90, 50, 50],
                        maxRange: [140, 255, 255]
                    }))
                    .via(new ContourDetectionNode()) // Custom
                    .to();
                ```
                {% endmarkdown %}
            </div>
        </div>
    </section>
    <section data-markdown data-auto-animate data-timing="20">

        {% markdown %}
            ## Example ...
        {% endmarkdown %}
        <div class="row">
            <div class="col-1">
                <img data-id="model-branch" src="images/model_branch_4.svg" width="100%"/>
            </div>
            <div class="col-11">
                {% markdown %}
                ```ts {7-16}
                GraphBuilder.create()
                    .from(new VideoSource({
                        autoPlay: true,
                        fps: 30,
                        throttleRead: true,
                        source: new CameraObject("sphero_video")
                    }).load("/dev/video2"))
                    .via(new ImageTransformNode({
                        src: [
                            new OpenCV.Point2(307, 120),
                            new OpenCV.Point2(1473, 87),
                            new OpenCV.Point2(1899, 891),
                            new OpenCV.Point2(20, 1024),
                        ],
                        height: 800,
                        width: 1040
                    }))
                    .via(new ColorMaskProcessing({
                        minRange: [90, 50, 50],
                        maxRange: [140, 255, 255]
                    }))
                    .via(new ContourDetectionNode()) // Custom
                    .to();
                ```
                {% endmarkdown %}
            </div>
        </div>
    </section>
    <section data-markdown data-auto-animate data-timing="20">

        {% markdown %}
            ## Example ...
        {% endmarkdown %}
        <div class="row">
            <div class="col-1">
                <img data-id="model-branch" src="images/model_branch_5.svg" width="100%"/>
            </div>
            <div class="col-11">
                {% markdown %}
                ```ts {17-20}
                GraphBuilder.create()
                    .from(new VideoSource({
                        autoPlay: true,
                        fps: 30,
                        throttleRead: true,
                        source: new CameraObject("sphero_video")
                    }).load("/dev/video2"))
                    .via(new ImageTransformNode({
                        src: [
                            new OpenCV.Point2(307, 120),
                            new OpenCV.Point2(1473, 87),
                            new OpenCV.Point2(1899, 891),
                            new OpenCV.Point2(20, 1024),
                        ],
                        height: 800,
                        width: 1040
                    }))
                    .via(new ColorMaskProcessing({
                        minRange: [90, 50, 50],
                        maxRange: [140, 255, 255]
                    }))
                    .via(new ContourDetectionNode()) // Custom
                    .to();
                ```
                {% endmarkdown %}
            </div>
        </div>
    </section>
    <section data-markdown data-auto-animate data-timing="20">

        {% markdown %}
            ## Example ...
        {% endmarkdown %}
        <div class="row">
            <div class="col-1">
                <img data-id="model-branch" src="images/model_branch_6.svg" width="100%"/>
            </div>
            <div class="col-11">
                {% markdown %}
                ```ts {21}
                GraphBuilder.create()
                    .from(new VideoSource({
                        autoPlay: true,
                        fps: 30,
                        throttleRead: true,
                        source: new CameraObject("sphero_video")
                    }).load("/dev/video2"))
                    .via(new ImageTransformNode({
                        src: [
                            new OpenCV.Point2(307, 120),
                            new OpenCV.Point2(1473, 87),
                            new OpenCV.Point2(1899, 891),
                            new OpenCV.Point2(20, 1024),
                        ],
                        height: 800,
                        width: 1040
                    }))
                    .via(new ColorMaskProcessing({
                        minRange: [90, 50, 50],
                        maxRange: [140, 255, 255]
                    }))
                    .via(new ContourDetectionNode()) // Custom
                    .to();
                ```
                {% endmarkdown %}
            </div>
        </div>
    </section>
    <section data-markdown data-auto-animate data-timing="20">
        {% markdown %}
            ## OpenCV: Contour detection
            ```ts {0-2,16-19}
            class ContourDetectionNode extends ProcessingNode<VideoFrame> {
                process(frame: VideoFrame): Promise<VideoFrame> {
                    return new Promise((resolve) => {
                        let contours = frame.image.findContours(
                            OpenCV.RETR_EXTERNAL, OpenCV.CHAIN_APPROX_SIMPLE);
                        if (contours.length >= 1) {
                            // Sort contours by area and select largest area as 'ball'
                            contours = contours.sort((a, b) => a.area - b.area);
                            const m = contours[0].moments();
                            const center = new OpenCV.Vec2(m.m10 / m.m00, m.m01 / m.m00);
                            // Use the center as the 2D pixel position
                            const position = new Absolute2DPosition(center.x, center.y);
                            position.unit = LengthUnit.CENTIMETER;
                            position.accuracy = Math.sqrt(contours[0].area);
                            frame.addObject(new DataObject("ball").setPosition(position));
                        }
                        resolve(frame);
                    });
                }
            }
            ```
        {% endmarkdown %}
    </section>
    <section data-markdown data-auto-animate data-timing="20">
        {% markdown %}
            ## OpenCV: Contour detection
            ```ts {}
            class ContourDetectionNode extends ProcessingNode<VideoFrame> {
                process(frame: VideoFrame): Promise<VideoFrame> {
                    return new Promise((resolve) => {
                        let contours = frame.image.findContours(
                            OpenCV.RETR_EXTERNAL, OpenCV.CHAIN_APPROX_SIMPLE);
                        if (contours.length >= 1) {
                            // Sort contours by area and select largest area as 'ball'
                            contours = contours.sort((a, b) => a.area - b.area);
                            const m = contours[0].moments();
                            const center = new OpenCV.Vec2(m.m10 / m.m00, m.m01 / m.m00);
                            // Use the center as the 2D pixel position
                            const position = new Absolute2DPosition(center.x, center.y);
                            position.unit = LengthUnit.CENTIMETER;
                            position.accuracy = Math.sqrt(contours[0].area);
                            frame.addObject(new DataObject("ball").setPosition(position));
                        }
                        resolve(frame);
                    });
                }
            }
            ```
        {% endmarkdown %}
    </section>
    <section data-markdown data-auto-animate data-timing="20">
        {% markdown %}
            ## OpenCV: Contour detection
            ```ts {10-14}
            class ContourDetectionNode extends ProcessingNode<VideoFrame> {
                process(frame: VideoFrame): Promise<VideoFrame> {
                    return new Promise((resolve) => {
                        let contours = frame.image.findContours(
                            OpenCV.RETR_EXTERNAL, OpenCV.CHAIN_APPROX_SIMPLE);
                        if (contours.length >= 1) {
                            // Sort contours by area and select largest area as 'ball'
                            contours = contours.sort((a, b) => a.area - b.area);
                            const m = contours[0].moments();
                            const center = new OpenCV.Vec2(m.m10 / m.m00, m.m01 / m.m00);
                            // Use the center as the 2D pixel position
                            const position = new Absolute2DPosition(center.x, center.y);
                            position.unit = LengthUnit.CENTIMETER; // Convert later
                            position.accuracy = Math.sqrt(contours[0].area);
                            frame.addObject(new DataObject("ball").setPosition(position));
                        }
                        resolve(frame);
                    });
                }
            }
            ```
        {% endmarkdown %}
    </section>
    <section data-markdown data-auto-animate data-timing="20">
        <video autoplay="true" width="100%" src="/media/2020-12_sphero-demo.mp4"></video>
    </section>
</section>

<section>
    <section data-markdown data-auto-animate data-timing="20">
        {% markdown %}
            ## Contributing and Future Work
            - Positioning algorithms
            - Process network communication
            - Bindings to other systems
            - (UI) abstractions for end-user authoring
            - Documentation and examples
            - Calibration and set-up utilities
        {% endmarkdown %}
    </section>
</section>

<section data-markdown data-auto-animate data-timing="30">
    <style>
    .resource-img {
        height: 50px;
        width: 50px;
        margin-left: none !important;
        display: inline-block !important;
        margin-top: 0 !important;
        margin-bottom: 0 !important;
    }
    .resource-link {
        margin-left: 1em;
        bottom: 12px;
    }
    </style>
    {% markdown %}
        ## Resources and Links
        <img class="resource-img" src="images/globe-solid.svg"><a class="resource-link" href="https://openhps.org">https://openhps.org</a>

        <img class="resource-img" src="images/github-brands.svg"><a class="resource-link" href="https://github.com/OpenHPS">https://github.com/OpenHPS</a>

        <img class="resource-img" src="images/npm-brands.svg"><a class="resource-link" href="https://npmjs.com/org/openhps">https://npmjs.com/org/openhps</a>

        <img class="resource-img" src="images/twitter-brands.svg"><a class="resource-link" href="https://twitter.com/OpenHPS">https://twitter.com/OpenHPS</a>
    {% endmarkdown %}

    <style>
    .qr-image {
        width: 200px;
        height: 200px;
    }
    </style>
    <div class="row" style="position: absolute; right: 50px; top: 130px; width: 30%;">
        <img style="width: 100%" src="images/contact.svg"/>
    </div>
    <div class="row" style="position: absolute; bottom: 7vh;">
        <img class="qr-image" src="../ipin2021/images/qr-code.svg"/>
    </div>
</section>