---
layout: presentation.njk
title: Indoor Positioning Using the OpenHPS Framework
presentation_title: Indoor Positioning Using the OpenHPS&nbsp;Framework
excerpt: "IPIN 2021 presentation for Indoor Positioning Using the OpenHPS Framework. In this paper we introduced OpenHPS and its use within the domain of indoor positioning."
author: <u>Maxim Van de Wynckel</u>, Beat Signer
logo: true
header_logo: /images/misc/openhps-logo.svg
affiliation: Web & Information Systems Engineering Lab</br>Vrije Universiteit Brussel
---
<section>
    <section data-markdown data-auto-animate data-timing="20">
        {% markdown %}
            ## What is OpenHPS?
            ### An Open Source Hybrid Positioning System
        {% endmarkdown %}
        <img width="67%" class="shadow" src="images/openhps-site.png"/>
        <aside class="notes">
        In this paper we introduced OpenHPS and its use within the domain of indoor positioning.

        OpenHPS is an open source hybrid positioning system developed in TypeScript that can run both on a server, 
        embedded device, smartphone or even the browser.
        </aside>
    </section>
    <section data-markdown data-auto-animate data-timing="40">
        {% markdown %}
            ## What is OpenHPS?
            ### An Open Source Hybrid Positioning System
            - Any technology
            - Any algorithm
            - Any use case
            - Flexibile processing and output
                - Accuracy over battery consumption, reliability, ...
            - Aimed towards
                - Developers
                - Researchers
        {% endmarkdown %}
        <aside class="notes">
        With OpenHPS we aim to offer a framework for developers and researchers that
        can support a variety of positioning technologies, algorithms and uses cases that are not just
        limited to indoor positioning. We also aim to offer a lot of flexibility in the outcome of the position,
        weither it is a developer wishing to offer the most accurate position, power optimized solution or
        simply a reliable system that can rougly (but reliably) indicate weither or not you are in a specific room.
        </aside>
    </section>
    <section data-markdown data-auto-animate data-timing="25">
        <h2 data-id="title-processnetworkdesign">Process Network Design</h2>
        <img width="90%" src="images/architecture.svg"/>
        <aside class="notes">
        When creating a positioning system, you obtain sensor data from multiple different sensors. You process the data until at some stage in your processing
        you merge the sensor data to obtain a position that you can show to your user.
        </aside>
    </section>
    <section data-markdown data-auto-animate data-timing="30">
        <h2 data-id="title-processnetworkdesign">Process Network Design ...</h2>
        <img height="30%" src="images/architecture.svg"/>
        <img height="45%" data-id="architecture-graph" src="images/architecture2.svg"/>
        <aside class="notes">
        OpenHPS focuses on this method of developing a positioning system by allowing the creation of a process network with graph topology. 
        We identify source nodes that provide data, processing nodes that process this data and finally sink nodes that store or display this data. 
        We call this complete process network from source to sink a positioning model.
        </aside>
    </section>
    <section data-markdown data-auto-animate data-timing="35">
        <h2 data-id="title-processnetworkdesign">Process Network Design ...</h2>
        <img height="70%" style="margin-top: 1em" data-id="architecture-graph" src="images/architecture3.svg"/>
        <aside class="notes">
        On top of this, every node in our process network has access to a range of 'Services' that either manage the persistent storage of data that is being handled or
        handle background processing of information that is already pushed through the network. Each service can be added to multiple positioning models, allowing data
        exchange between the calibration part of a process network and the actual online stage used on a production ready system.
        </aside>
    </section>
    <section data-markdown data-auto-animate data-timing="40">
        <h2 data-id="title-modularity">Modularity</h2>
        <img width="100%" style="margin-top: 2em" src="images/stack.svg"/>
        <aside class="notes">
        By creating this process network design with shared services we allow for a very modular ecosystem where individual nodes or services
        can be reused throughout multiple different packages. Our core component offers the main concepts
        needed to design a process network for processing sensor data to an output position while other components
        handle the communication, data storage or add additional algorithms.
        </aside>
    </section>
    <section data-markdown data-auto-animate data-visibility="uncounted" data-timing="0">
        <h2 data-id="title-modularity">Modularity ...</h2>
        {% markdown %}
            **Communication**\
            Socket, MQTT, REST API, ...

            **Data Storage**\
            MongoDB, LocalStorage, RDF, ...

            **Positioning Algorithms**\
            IMU, fingerprinting, OpenVSLAM, ...

            **Abstractions**\
            Geospatial, location-based services, geojson, ...

            **Other**\
            React-Native, NativeScript, Sphero, ...
        {% endmarkdown %}
        <aside class="notes">
        [OPTIONAL SLIDE] We already offer many different modules for the more common positioning techinques. In this paper we will use a socket module
        that handles the communication from a mobile application to a server, MongoDB to persist our fingerprinting data and
        fixed beacon locations, the IMU module for pedestrian dead reckoning, symbolic spaces that we will discuss later and finally react-native
        that offers source nodes for sensor data on a smartphone.
        </aside>
    </section>
</section>

<section>
    <section data-markdown data-auto-animate data-timing="30">
        {% markdown %}
            ## Data Processing
        {% endmarkdown %}
        <img height="80%" src="images/datatypes.svg" class="logo">
        <aside class="notes">
        The data that we process in our process network is an important aspect of the core of OpenHPS.
        We have knowledge, raw data and processed data. Knowledge is everything that we know - such as sensors, radio transmitters
        or building layouts. Raw data is unprocessed data that we want to process, such as individual video or image frames, unfiltered
        accelerometer data. And finally, we have the processed data that will eventually become knowledge.
        </aside>
    </section>
    <section data-markdown data-auto-animate data-timing="40">
        {% markdown %}
            ## DataObject
            
        {% endmarkdown %}
        <img height="75%" src="images/dataobject.svg"/>
        <aside class="notes">
        We represent this knowledge and processed data under the term dataobject.

        A data object is any spatial entity that is relevant for the positioning of other data objects. A data object can be considered
        a camera object tracking another object - or it can be a phone that is responsible itself for
        the data collection.

        Each data object can have a hierarchical relation with other objects.
        </aside>
    </section>
    <section data-markdown data-auto-animate data-timing="30">
        {% markdown %}
            ## Absolute and Relative Positions
            **Absolute**
            - 2D, 3D, Geographical, ...

            **Relative**
            - Distance, angle, velocity, RSSI, ...
            - Relative to another *object*
        {% endmarkdown %}
        <aside class="notes">
        These spatial objects can have an absolute or relative position. An absolute position
        represents a fixed position in space. We support 2D, 3D and geographical positions.

        Relative positions are relative to another object and can be expressed in 
        distance, angle, velocity or a more abstract RSS.
        </aside>
    </section>
    <section data-markdown data-auto-animate data-visibility="hidden">
        {% markdown %}
            ## ReferenceSpace
            *An absolute position can be relative to a reference space.*
        {% endmarkdown %}
        <img width="70%" src="images/referencespace.svg"/>
        <aside class="notes">
        Reference spaces are data objects that represent a transformation of a position. When setting the position or orientation of an object
        you can optionally specify the reference space.

        Similar to data objects, reference spaces can have a hierarchical relation with other objects.
        </aside>
    </section>
    <section data-markdown data-auto-animate data-timing="80">
        <h2 data-id="title-dataframe">DataFrame</h2>
        <img height="75%" src="images/dataframe_single.svg"/>
        <aside class="notes">
        In order to contain our processed data with our raw data, we create a new Data frame.

        Data frames are individual captured moments of sensor data processed in the positioning system. They are the
        enveloppes transmitted through the process network and are usually created
        by a source object and can contain a set of relevant objects or raw sensor data that are added or modified by nodes in our network

        In this example image we have a video frame that is captured by a camera object. It contains the actual image
        of that frame and a set of objects that were detected inside this frame.
        </aside>
    </section>
    <section data-markdown data-auto-animate data-visibility="uncounted" data-timing="0">
        <h2 data-id="title-dataframe">DataFrame ...</h2>
        {% markdown %}
            ### Pushing Data

        {% endmarkdown %}
        <img width="100%" src="images/nodes-push.svg"/>
        <aside class="notes">
        [OPTIONAL SLIDE] 
        </aside>
    </section>
    <section data-markdown data-auto-animate data-visibility="uncounted" data-timing="0">
        <h2 data-id="title-dataframe">DataFrame ...</h2>
        {% markdown %}
            ### Pulling Data

        {% endmarkdown %}
        <img width="100%" src="images/nodes-pull.svg"/>
        <aside class="notes">
        [OPTIONAL SLIDE] 
        </aside>
    </section>
    <section data-markdown data-auto-animate data-visibility="uncounted" data-timing="0">
        <h2 data-id="title-dataframe">DataFrame ...</h2>
        {% markdown %}
            ### Pushing Error

        {% endmarkdown %}
        <img width="100%" src="images/nodes-push-error.svg"/>
        <aside class="notes">
        [OPTIONAL SLIDE] Errors can of course occur in the process network. When this happens, the push is rejected instead of resolved - and the
        last node that performed the failed push will emit an error that will propagate upstream so the upstream nodes
        are aware of the error. It is down to the developer to decide how these errors are handled.
        </aside>
    </section>
</section>

<section>
    <section data-markdown data-auto-animate data-timing="30">
        {% markdown %}
            ## SymbolicSpace
            *An object that semantically defines a space*
        {% endmarkdown %}
        <div class="row">
            <div class="col-7">
                {% markdown %}
                - Spatial hierarchy
                - Graph connectivity with other spaces
                - Geocoding
                - GeoJSON compatibility
                - Can be used as a location
                {% endmarkdown %}
            </div>
            <div class="col-5">
                <img width="100%" src="images/geojson.svg"/>
            <div>
        </div>
        <aside class="notes">
        Symbolic spaces extend the concept of reference spaces ....
        </aside>
    </section>
    <section data-markdown data-auto-animate data-visibility="uncounted" data-timing="0">
        {% markdown %}
            ## SymbolicSpace ...
        {% endmarkdown %}
        <div class="row">
            <div class="col-5">
                <img width="100%" style="margin-top: 1em" src="images/symbolicspace.svg"/>
            </div>
            <div class="col-7">
            {% markdown %}
                ```ts
                const building = new Building("PL9")
                    .setBounds({
                        topLeft: new GeographicalPosition(
                                50.8203, 
                                4.3922),
                        width: 46.275, 
                        height: 37.27, 
                        rotation: -34.04
                    });
                
                const floor = new Floor("PL9.3")
                    .setBuilding(building)
                    .setFloorNumber(3);
                
                const office = new Room("PL9.3.58")
                    .setFloor(floor)
                    .setBounds([
                        new Absolute2DPosition(4.75, 31.25),
                        new Absolute2DPosition(8.35, 37.02),
                    ]);
                ```
            {% endmarkdown %}
            </div>
        </div>
        <aside class="notes">
        [OPTIONAL SLIDE] 
        </aside>
    </section>
</section>

<section>
    <section data-markdown data-auto-animate data-timing="40">
        <h2 data-id="title-lbs">Location-based Service</h2>
        <img data-id="lbs-image" src="images/lbs1_h.svg" style="width: 100%;"/>
        <aside class="notes">
        The location based service creates an abstraction of the underlying process network.
        It offers a similar API structure than well known APIs. When a user simply requests the
        current position of a person with a set of options such as the maximum age of the position,
        the LBS will first check if a position is stored in the storage. If not, it will perform a pull
        on the sink of the process network.
        </aside>
    </section>
    <section data-markdown data-auto-animate data-visibility="uncounted">
        <h2 data-id="title-lbs">Location-based Service ...</h2>
        <img data-id="lbs-image" src="images/lbs2_h.svg" style="width: 100%;"/>
        <aside class="notes">
        Setting the current position will create an abstraction that uses the current storage
        method to store the new position. This position can then be used by the process network.
        </aside>
    </section>
    <section data-markdown data-auto-animate data-timing="20">
        <h2 data-id="title-lbs">Location-based Service ...</h2>
        <img data-id="lbs-image" src="images/lbs3_h.svg" style="width: 100%;"/>
        <aside class="notes">
        Finally, watching for position changes will listen for new dataframe pushes on the sink
        of the process network. If the positioning system does not update frequently, it will
        trigger a pull every interval.
        </aside>
    </section>
</section>

<section>
    <section data-markdown data-auto-animate data-timing="30">
        {% markdown %}
            ## Demonstration
            - Indoor positioning **use case**
            - Use **existing techniques**
            - Validation of **flexibility** and modularity
        {% endmarkdown %}
        <img src="images/model.svg" style="width: 98%;"/>
        <aside class="notes">
        In order to demonstrate that OpenHPS can be used for indoor positioning scenario's we created
        a demonstration using known and common indoor positioning techniques.

        This demonstrator mainly serves as a validation that we can use a process network design for 
        developing a modular indoor positioning system.

        Our demonstration consists out of two applications and a server
        for handling a common implementation with IMU data, WLAN access points and BLE beacons. 
        </aside>
    </section>
    <section data-markdown data-auto-animate data-timing="30">
        <h2 data-id="title-positioningmodel">Positioning Model</h2>
        <div class="row" style="margin-top: 3rem;">
            <div class="col-9">
                <img src="images/model-offline.svg" width="100%"/>
            </div>
            <div class="col-3">
                <img src="images/smartphone-offline.png" width="100%"/>
            </div>
        </div>
        <aside class="notes">
        The offline stage application handles the set-up and uses source nodes from our react native component; one that scans for wifi access points,
        one that scans for BLE tags and finally another custom source node for the user location input using
        the smartphone. The user selected location is merged with the scan results of the BLE and WLAN device and
        send to a sink node that acts as a socket interface to the server.
        </aside>
    </section>
       <section data-markdown data-auto-animate data-timing="20">
        <h2 data-id="title-positioningmodel">Positioning Model ...</h2>
        <img src="images/model-server-offline.svg" style="width: 100%;"/>
        <aside class="notes">
        On the server this data is received and stored to a shared fingerprinting services
        that extracts the features. The services are accessible by the online stage
        of the positioning.
        </aside>
    </section>
    <section data-markdown data-auto-animate data-timing="15">
        <h2 data-id="title-positioningmodel">Positioning Model ...</h2>
        <img src="images/model-online-1.svg" style="width: 100%; margin-top: 7rem;"/>
        <aside class="notes">
        The online stage application uses a wifi and ble source similar to the offline stage application. 
        The WLAN and BLE scan results are merged and send to the server using a socket connection
        </aside>
    </section>
    <section data-markdown data-auto-animate data-timing="30">
        <h2 data-id="title-positioningmodel">Positioning Model ...</h2>
        <img src="images/model-server-online.svg" style="width: 95%;"/>
        <aside class="notes">
        On the server we use three positioning techniques to obtain a position from this WLAN and BLE data.
        We process the wlan data with a knn fingerprinting algorithm and we perform both ble fingerprinting and
        ble multilateration using the known positions of the beacons in our building. These three outputs are merged and
        send back to the online application.
        </aside>
    </section>
    <section data-markdown data-auto-animate data-timing="60">
        <h2 data-id="title-positioningmodel">Positioning Model ...</h2>
        <img src="images/model-online-2.svg" style="width: 100%; margin-top: 7rem;"/>
        <aside class="notes">
        In the online application an IMU source node is used together with pedestrian dead reckoning. The dead reckoning
        is applied on the last calculated position and is used as an intermediate calculate between responses from the server.
        The known velocity of the user is applied to the response from the server in the Velocity processing node named 'a' in order
        to account for the time it took between the wlan or ble scan and the response from the server. Once this processed
        server response is merged with the imu data, it is displayed to the user. An addition feedback loop is created for the predicted
        movement based on the current momentum.
        </aside>
    </section>
    <section data-markdown data-auto-animate data-visibility="uncounted" data-timing="0">
        {% markdown %}
            ## Positioning Model
            ### Online App
        {% endmarkdown %}
        <img src="images/pdr.svg" style="height: 70%;"/>
        <aside class="notes">
        [OPTIONAL SLIDE] 
        </aside>
    </section>
    <section data-markdown data-auto-animate data-visibility="uncounted" data-timing="0">
        {% markdown %}
            ## Positioning Model
            ### Online App
        {% endmarkdown %}
        {% markdown %}
            ```ts {10-20}
            ModelBuilder.create()
                .addShape(GraphBuilder.create()
                    .from(new IMUSourceNode({
                        source: new DataObject(phoneUID),
                        interval: 20,
                        sensors: [
                            SensorType.ACCELEROMETER, 
                            SensorType.ORIENTATION
                        ]
                    }))
                    .via(new SMAFilterNode(
                        frame => [frame, "acceleration"], 
                        { taps: 10 }
                    ))
                    .via(new GravityProcessingNode({
                        method: GravityProcessingMethod.ABSOLUTE_ORIENTATION
                    }))
                    .via(new PedometerProcessingNode({
                        minConsecutiveSteps: 1, 
                        stepSize: 0.40
                    }))
                    .via(new VelocityProcessingNode())
                    .to("pedometer-output"))
                .build();
            ```
        {% endmarkdown %}
        <aside class="notes">
        [OPTIONAL SLIDE] 
        </aside>
    </section>
</section>

<section>
    <section data-markdown data-auto-animate data-timing="40">
        {% markdown %}
            ## Dataset
        {% endmarkdown %}
        <img data-id="dataset-image" src="images/datapoints.svg" style="width: 75%;"/>
        <aside class="notes">
        For this paper we created a new public dataset recorded using OpenHPS. Our dataset consists of multiple
        training and test data points that are each recorded in four directions. For each orientation we included WLAN
        scan results and BLE scan results from the 11 beacons with known position. We also provide several trajectories
        that also include IMU sensor data.
        </aside>
    </section>
    <section data-markdown data-auto-animate data-visibility="uncounted" data-timing="0">
        {% markdown %}
            ## Dataset ...
            **Total BLE Beacons**: 11\
            **Total detected WLAN access points**: 220\
            **Total stable WLAN access points**: 199

            ||Training|Test|
            |-|-|-|
            |**Datapoints**|110|30|
            |**Total fingerprints**|440|120|
            |**Duration <small>(per orientation)</small>**|20s|20s|
            |**Avg. WLAN Scans <small>(per fingerprint)</small>**|6|6|
            |**Avg. BLE Advertisements <small>(per fingerprint)</small>**|16|15|
        {% endmarkdown %}
        <aside class="notes">
        [OPTIONAL SLIDE] We have 110 training datapoints and 30 datapoints, which in total gives us 440 training fingerprints
        and 120 test datapoints. For each fingerprint we waited 20 seconds to collect wifi and ble scan results giving
        us an average of 6 complete wlan scans and 15 individual ble advertisements.
        </aside>
    </section>
</section>

<section>
    <section data-markdown data-auto-animate data-timing="70">
        <style>
        .results table {
            font-size: 80%;
            margin-top: 2em;
        }

        .results tr > td:last-of-type {
            background-color: #f5f5f5;
        }

        .results tr > th:last-of-type {
            background-color: #f5f5f5;
        }
        </style>
        <h2 data-id="title-validationresults">Validation Results</h2>
        <h3>Static Positioning</h3>
        <div class="results">
        {% markdown %}
            ||**WLAN <small>fingerprinting</small>**|**BLE <small>fingerprinting</small>**|**BLE <small>multilateration</small>**|**Fusion**|
            |-|-|-|-|-|
            |*failed points*|0|6|12|0|
            |*average error*|1.23 m|3.23 m|4.92 m|1.37 m|
            |*minimum error*|0.01 m|0.17 m|0.74 m|0.01 m|
            |*maximum error*|4.77 m|15.39 m|19.26 m|9.75 m|
            |*hit rate*|95.82 %|80.83 %|52.50 %|<span class="highlight">96.67 %</span>|
        {% endmarkdown %}
        </div>
        <aside class="notes">
        Using this demonstrator and the dataset that we recorded with the offline part of the demonstrator - we conducted a validation of our
        positioning system. In this compared wifi fingerprinting, bluetooth fingerprinting using the 11 beacons and ble multilateration.
        While WLAN fingerprinting offers the best results, we demonstrate that the fusion with bluetooth beacons
        provides a higher hit rate. This higher hit rate is because we were able to give a higher weight to the Bluetooth positioning methods
        whenever the user was close to one or more beacons.
        </aside>
    </section>
    <section data-markdown data-auto-animate>
        <h2 data-id="title-validationresults">Validation Results ...</h2>
        <h3>Trajectories</h3>
        <img data-id="trajectories" src="images/trajectories_flipped.svg" style="width: 90%;"/>
        <aside class="notes">
        
        </aside>
    </section>
    <section data-markdown data-auto-animate data-timing="70">
        <style>
        .results table {
            margin-top: 0;
        }

        .results.trajectories tr > td:last-of-type {
            background-color: #0034ad;
            color: white;
        }

        .results.trajectories tr > th:last-of-type {
            background-color: #0034ad;
            color: white;
        }
        </style>
        <h2 data-id="title-validationresults">Validation Results ...</h2>
        <h3>Trajectories</h3>
        <div class="results trajectories">
        {% markdown %}
            ||**WLAN + BLE**|**WLAN + BLE + IMU**|
            |-|-|-|
            |*average error*|3.28 m|1.26 m|
            |*maximum error*|9.60 m|3.10 m|
            |*average update frequency*|3.04 s|0.52 s|
        {% endmarkdown %}
        </div>
        <img data-id="trajectories" src="images/trajectories_flipped_2.svg" style="width: 100%;"/>
        <aside class="notes">

        </aside>
    </section>
</section>

<section data-markdown data-auto-animate data-timing="30">
    <style>
    .qr-image {
        width: 200px;
        height: 200px;
    }
    </style>
    {% markdown %}
        ## Contributions and Conclusions
        - OpenHPS: **open source** framework for hybrid positioning
            - Aimed towards **developers** and **researchers**
        - **Abstractions** such as location-based services and spaces
        - Configurable and interchangeable **nodes** and **services**
        - Validation of an indoor positioning use case
        - **Public dataset** with multiple orientations
    {% endmarkdown %}
    <div class="row" style="position: absolute; bottom: 7vh;">
        <div class="col-3">
            <img class="qr-image" src="images/qr-code.svg"/>
        </div>
        <div class="col-7">
            {% markdown %}
            \
            *Visit [https://openhps.org](https://openhps.org) for additional resources,
            documentation, source code and more!*
            {% endmarkdown %}
        </div>
    </div>
    <aside class="notes">
    We have discussed OpenHPS as an open source positioning system framework
    aimed towards developers and researchers.

    We validated this using a demonstrator and a new dataset that is available to the public.

    Thank you for your attention
    </aside>
</section>