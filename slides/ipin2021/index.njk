---
layout: presentation.njk
title: Indoor Positioning Using the OpenHPS Framework
author: <u>Maxim Van de Wynckel</u>, Beat Signer
logo: true
header_logo: /images/misc/openhps-logo.svg
---
<style>
.qr-image {
    width: 200px;
    height: 200px;
}
</style>

<section>
    <section data-markdown data-auto-animate>
        {% markdown %}
            ## What is OpenHPS?
            ### An open source hybrid positioning system
        {% endmarkdown %}
        <img width="70%" class="shadow" src="images/openhps-site.png"/>
        <aside class="notes">
        OpenHPS is an open source hybrid positioning system developed in TypeScript that can run both on a server, 
        embedded device, smartphone or even the browser.
        </aside>
    </section>
    <section data-markdown data-auto-animate>
        {% markdown %}
            ## What is OpenHPS?
            ### An open source hybrid positioning system
            - Support any technology
                - From RF-based to cameras or LIDAR scanners
            - Support any algorithm
                - From basic fingerprinting to GPU intensive computer vision
            - Support any use case
                - Indoor or outdoor positioning or navigation
                - Tabletop positioning
            - Flexibility of outcome
                - Accuracy over battery consumption, reliability, ..
            - Aimed towards ...
                - **Developers**: For creating production-ready positioning systems
                - **Researchers**: For rapid prototyping new algorithms and fusion techniques
        {% endmarkdown %}
        <aside class="notes">
        With OpenHPS we aim to offer a framework for developers and researchers that
        can support a variety of positioning technologies, algorithms and uses cases that are not just
        limited to indoor positioning. We also aim to offer a lot of flexibility in the outcome of the position,
        weither it is a developer wishing to offer the most accurate position, power optimized solution or
        simply a reliable system that can rougly indicate weither or not you are in a specific room.
        </aside>
    </section>
    <section data-markdown data-auto-animate>
        {% markdown %}
            ## What is OpenHPS?
            ### Process network design
            
        {% endmarkdown %}

        <img width="90%" src="images/architecture.svg"/>
        <aside class="notes">
        When creating a positioning system, you obtain sensor data from multiple different sensors. You process the data until at some stage in your processing
        you merge the sensor data in order to obtain a more reliable or accurate position that you can show to your user.
        </aside>
    </section>
    <section data-markdown data-auto-animate>
        {% markdown %}
            ## What is OpenHPS?
            ### Process network design
            
        {% endmarkdown %}
        <img height="30%" src="images/architecture.svg"/>
        <img height="30%" src="images/architecture2.svg"/>
        <aside class="notes">
        OpenHPS focuses on this method of developing a positioning system by allowing the creation of a process network with graph topology. 
        We identify source nodes that provide data, processing nodes that process this data and finally sink nodes that store or display this data. 
        We call this complete process network from source to sink a positioning model.
        </aside>
    </section>
    <section data-markdown data-auto-animate>
        {% markdown %}
            ## What is OpenHPS?
            ### Shared services
            
        {% endmarkdown %}
        <img height="60%" src="images/architecture3.svg"/>
        <aside class="notes">
        On top of this, every node in our process network has access to a range of 'Services' that either manage the persistent storage of data that is being handled or
        handle background processing of information that is already pushed through the network. Each service can be added to multiple positioning models, allowing data
        exchange between the calibration part of a process network and the actual online stage used on a production ready system.
        </aside>
    </section>
    <section data-markdown data-auto-animate>
        {% markdown %}
            ## What is OpenHPS?
            ### Modularity
        {% endmarkdown %}
        <img width="95%" src="images/stack.svg"/>
        <aside class="notes">
        By creating this process network design with shared services we allow for a very modular ecosystem where individual nodes or services
        can be reused throughout multiple different packages. Our core component offers the main concepts
        needed to design a process network for processing sensor data to an output position, communication
        modules allow for remote nodes or parts of the process network. Modules offering positioning techniques
        add extra algorithms or interfaces to other technologies such as fingerprinting or links
        to a visual SLAM algorithm. Data storage modules offer persistence of data and finally a module can
        also create an entire abstraction of the underling process network so it can be more easily used by developers.
        </aside>
    </section>
    <section data-markdown data-auto-animate>
        {% markdown %}
            ## What is OpenHPS?
            ### Modularity
            **Communication**\
            Socket, MQTT, REST API, ...

            **Data Storage**\
            MongoDB, LocalStorage, ...

            **Positioning Algorithms**\
            IMU, fingerprinting, OpenVSLAM, ...

            **Abstractions**\
            Symbolic spaces, location based services, geojson ...

            **Misc**\
            NativeScript, React-Native, Sphero ...
        {% endmarkdown %}
        <aside class="notes">
        We already offer many different modules for the more common positioning techinques. In this paper we will use a socket module
        that handles the communication from a mobile application to a server, MongoDB to persist our fingerprinting data and
        fixed beacon locations, the IMU module for pedestrian dead reckoning, symbolic spaces that we will discuss later and finally react-native
        that offers source nodes for sensor data on a smartphone.
        </aside>
    </section>
</section>

<section>
    <section data-markdown data-auto-animate>
        {% markdown %}
            ## Architecture
            ### Design principles
            - Tracked actor
            - Tracking actor
            - Computing actor
            - Calibration actor
        {% endmarkdown %}
        <aside class="notes">
        Our framework architecture evolved around four actors that we can identify in all positioning system implementations. A tracked
        actor represents the person or object that is being tracked - it is the actor that we want to know the position for.

        A tracking actor is the device responsible for tracking a tracked actor. It can be an external camera tracking how a person
        moves through a building, or simply sensor data obtained by the tracked actor itself.

        The computing actor is a type of actor that computes the information obtained by a tracking actor to a position for the tracked actor.

        Finally, a calibration actor is the object or device responsible for setting up the system. This can be a manual configuration by a developer or
        a training or calibration needed when setting up the system.
        </aside>
    </section>
    <section data-markdown data-auto-animate>
        {% markdown %}
            ## Architecture
            ### ```DataObject```
            
        {% endmarkdown %}

        <aside class="notes">
        We translated these design principles into data objects. A data object is an entity
        that is relevant for the positioning of other data objects. A data object can be considered
        a camera object tracking another object - or it can be a phone that is responsible itself for
        the data collection.

        Each data object can have a hierarchical relation with other objects.
        </aside>
    </section>
    <section data-markdown data-auto-animate>
        {% markdown %}
            ## Architecture
            ### ```DataFrame```

        {% endmarkdown %}
        <img width="100%" src="images/dataframe.svg"/>
        <aside class="notes">
        Data frames are individual captured moments of sensor data processed in the positioning system. They are created
        by a source object and can contain a set of relevant objects or raw sensor data.

        In this example image we have a video frame that is captured by a camera object. It contains the actual image
        of that frame and a set of objects that were detected inside this frame.

        In the second example we have a simple IMU data frame that captures the currently acceleration of a sensor object.

        And finally, an RF data frame that is created by a receiver and contains relative signal strengths to detected
        access points that are represented as individual data objects.
        </aside>
    </section>
    <section data-markdown data-auto-animate>
        {% markdown %}
            ## Architecture
            ### ```ReferenceSpace```
            *An absolute position is relative to a reference space. By default this is the global reference space (i.e. world).*
        {% endmarkdown %}
        <aside class="notes">
        
        </aside>
    </section>
        <section data-markdown data-auto-animate>
        {% markdown %}
            ## Architecture
            ### Pushing data

        {% endmarkdown %}
        <img width="100%" src="images/nodes-push.svg"/>
        <aside class="notes">
        
        </aside>
    </section>
    <section data-markdown data-auto-animate>
        {% markdown %}
            ## Architecture
            ### Pulling data

        {% endmarkdown %}
        <img width="100%" src="images/nodes-pull.svg"/>
        <aside class="notes">
        
        </aside>
    </section>
    <section data-markdown data-auto-animate>
        {% markdown %}
            ## Architecture
            ### Pushing error

        {% endmarkdown %}
        <img width="100%" src="images/nodes-push-error.svg"/>
        <aside class="notes">
        Errors can of course occur in the process network. When this happens, the push is rejected instead of resolved - and the
        last node that performed the failed push will emit an error that will propagate upstream so the upstream nodes
        are aware of the error. It is down to the developer to decide how these errors are handled.
        </aside>
    </section>
</section>

<section>
    <section data-markdown data-auto-animate>
        {% markdown %}
            ## Indoor Positioning
            - Symbolic Spaces
            - Location-based Service
        {% endmarkdown %}
    </section>
    <section data-markdown data-auto-animate>
        {% markdown %}
            ## Indoor Positioning
            ### ```SymbolicSpace```
            *An object that defines a space*
            - Hierarchical relation with other spaces (or other objects)
            - Connected relation with other spaces (or other objects)
        {% endmarkdown %}
    </section>
</section>

<section>
    <section data-markdown data-auto-animate>
        {% markdown %}
            ## Dataset
        {% endmarkdown %}
        <img data-id="dataset-image" src="images/datapoints.svg" style="width: 75%;"/>
        <aside class="notes">
        For this paper we created a new public dataset recorded using OpenHPS. Our dataset consists of multiple
        training and test data points that are each recorded in four directions. For each orientation we included WLAN
        scan results and BLE scan results from the 11 beacons with known position. We also provide several trajectories
        that also include IMU sensor data.
        </aside>
    </section>
    <section data-markdown data-auto-animate>
        {% markdown %}
            ## Dataset
            **Total BLE Beacons**: 11\
            **Total detected WLAN access points**: 220\
            **Total stable WLAN access points**: 199

            ||Training|Test|
            |-|-|-|
            |**Datapoints**|110|30|
            |**Total fingerprints**|440|120|
            |**Duration <small>(per orientation)</small>**|20s|20s|
            |**Avg. WLAN Scans <small>(per fingerprint)</small>**|6|6|
            |**Avg. BLE Advertisements <small>(per fingerprint)</small>**|16|15|
        {% endmarkdown %}
        <aside class="notes">
        We have 110 training datapoints and 30 datapoints, which in total gives us 440 training fingerprints
        and 120 test datapoints. For each fingerprint we waited 20 seconds to collect wifi and ble scan results.
        </aside>
    </section>
</section>

<section>
    <section data-markdown data-auto-animate>
        {% markdown %}
            ## Demonstration
            ### Goals
            - Use OpenHPS for indoor positioning
            - Flexibility of changing the positioning model
                - Focus on accuracy
                - Focus on reliability (symbolic space hit rate)
        {% endmarkdown %}
        <aside class="notes">
        In order to demonstrate that OpenHPS can be used for indoor positioning scenario's we created
        a demonstration using known and common indoor positioning techniques.

        This demonstrator mainly serves as a validation that we can use a process network design for 
        developing a modular indoor positioning system. We demonstrate the flexibility by 
        </aside>
    </section>
</section>
<section>
    <section data-markdown data-auto-animate>
        {% markdown %}
            ## Demonstration
            ### Positioning model
        {% endmarkdown %}
        <img src="images/model.svg" style="width: 95%;"/>
        <aside class="notes">
        For demonstrating the use of OpenHPS in an indoor positioning scenario we created a positioning system consisting out of two applications and a server
        for handling a common implementation with IMU data, WLAN access points and BLE beacons. 
        </aside>
    </section>
    <section data-markdown data-auto-animate>
        {% markdown %}
            ## Demonstration
            ### Positioning model (Offline App)
        {% endmarkdown %}
        <div class="row">
            <div class="col-3">
                <img src="images/smartphone-offline.png" height="100%"/>
            </div>
            <div class="col-8">
                <img src="images/model-offline.svg" width="100%"/>
            </div>
        </div>
        <aside class="notes">
        The offline stage application handles the set-up and uses source nodes from our react native component; one that scans for wifi access points,
        one that scans for BLE tags and finally another custom source node for the user location input using
        the smartphone. The user selected location is merged with the scan results of the BLE and WLAN device and
        send to a sink node that acts as a socket interface to the server.
        </aside>
    </section>
       <section data-markdown data-auto-animate>
        {% markdown %}
            ## Demonstration
            ### Positioning model (Server #1)
        {% endmarkdown %}
        <img src="images/model-server-offline.svg" style="width: 90%;"/>
        <aside class="notes">
        On the server this data is received and stored to a shared fingerprinting services
        that extracts the features.
        </aside>
    </section>
    <section data-markdown data-auto-animate>
        {% markdown %}
            ## Demonstration
            ### Positioning model (Online app #1)
        {% endmarkdown %}
        <img src="images/model-online-1.svg" style="width: 90%;"/>
        <aside class="notes">
        The online stage application uses a wifi and ble source similar to the offline stage application. 
        The WLAN and BLE scan results are merged and send to the server using a socket connection
        </aside>
    </section>
    <section data-markdown data-auto-animate>
        {% markdown %}
            ## Demonstration
            ### Positioning model (Server #2)
        {% endmarkdown %}
        <img src="images/model-server-online.svg" style="width: 90%;"/>
        <aside class="notes">
        On the server we use three positioning techniques to obtain a position from this WLAN and BLE data.
        We process the wlan data with a knn fingerprinting algorithm and we perform both ble fingerprinting and
        ble multilateration using the known positions of the beacons in our building. These three outputs are merged and
        send back to the online application.
        </aside>
    </section>
    <section data-markdown data-auto-animate>
        {% markdown %}
            ## Demonstration
            ### Positioning model (Online app #2)
        {% endmarkdown %}
        <img src="images/model-online-2.svg" style="width: 90%;"/>
        <aside class="notes">
        In the online application an IMU source node is used together with pedestrian dead reckoning. The dead reckoning
        is applied on the last calculated position and is used as an intermediate calculate between responses from the server.
        The known velocity of the user is applied to the response from the server in the Velocity processing node named 'a' in order
        to account for the time it took between the wlan or ble scan and the response from the server. Once this processed
        server response is merged with the imu data, it is displayed to the user. An addition feedback loop is created for the predicted
        movement based on the current momentum.
        </aside>
    </section>
    <section data-markdown data-auto-animate>
        {% markdown %}
            ## Demonstration
            ### Positioning model (Online app #3)
        {% endmarkdown %}
        <img src="images/pdr.svg" style="height: 70%;"/>
        <aside class="notes">
        
        </aside>
    </section>
    <section data-markdown data-auto-animate>
        {% markdown %}
            ## Demonstration
            ### Positioning model (Online app #3)
        {% endmarkdown %}
        {% markdown %}
            ```ts {10-20}
            ModelBuilder.create()
                .addShape(GraphBuilder.create()
                    .from(new IMUSourceNode({
                        source: new DataObject(phoneUID),
                        interval: 20,
                        sensors: [
                            SensorType.ACCELEROMETER, 
                            SensorType.ORIENTATION
                        ]
                    }))
                    .via(new SMAFilterNode(
                        frame => [frame, "acceleration"], 
                        { taps: 10 }
                    ))
                    .via(new GravityProcessingNode({
                        method: GravityProcessingMethod.ABSOLUTE_ORIENTATION
                    }))
                    .via(new PedometerProcessingNode({
                        minConsecutiveSteps: 1, 
                        stepSize: 0.40
                    }))
                    .via(new VelocityProcessingNode())
                    .to("pedometer-output"))
                .build();
            ```
        {% endmarkdown %}
        <aside class="notes">
        
        </aside>
    </section>
</section>


<section>
    <section data-markdown data-auto-animate>
        {% markdown %}
            ## Demonstration
            ### Static Positioning
        {% endmarkdown %}
        <aside class="notes">
        
        </aside>
    </section>
</section>

<section>
    <section data-markdown data-auto-animate>
        {% markdown %}
            ## Demonstration
            ### Trajectories
        {% endmarkdown %}
        <img data-id="trajectories" src="images/trajectories_thesis.svg" style="width: 90%;"/>
        <aside class="notes">
        
        </aside>
    </section>
    <section data-markdown data-auto-animate>
        {% markdown %}
            ## Demonstration
            ### Trajectories
        {% endmarkdown %}
        <img data-id="trajectories"src="images/trajectories_flipped.svg" style="width: 90%;"/>
        <aside class="notes">
        
        </aside>
    </section>
    <section data-markdown data-auto-animate>
        {% markdown %}
            ## Demonstration
            ### Trajectories
            **WLAN Fingerprinting**

        {% endmarkdown %}
        <aside class="notes">
        
        </aside>
    </section>
</section>

<section>
    <section data-markdown data-auto-animate>
        {% markdown %}
            ## Conclusions & Future Work
            - **Open Source** framework for Hybrid Positioning
                - Aimed towards **developers** and **researchers**
            - Public **dataset** with multiple orientations
        {% endmarkdown %}
        <img class="qr-image" src="images/qr-code.svg"/>
    </section>
</section>