---
layout: presentation.njk
title: Indoor Positioning Using the OpenHPS Framework
author: <u>Maxim Van de Wynckel</u>, Beat Signer
logo: true
header_logo: /images/misc/openhps-logo.svg
---
<style>
.qr-image {
    width: 200px;
    height: 200px;
}
</style>

<!-- Array.from(document.querySelectorAll("section[data-timing]").values()).map(e => e.getAttribute("data-timing")).reduce((a, b) => +a + +b) / 60 -->

<section>
    <section data-markdown data-auto-animate data-timing="20">
        {% markdown %}
            ## What is OpenHPS?
            ### An open source hybrid positioning system
        {% endmarkdown %}
        <img width="70%" class="shadow" src="images/openhps-site.png"/>
        <aside class="notes">
        In this paper we introduced OpenHPS and its use within the domain of indoor positioning.

        OpenHPS is an open source hybrid positioning system developed in TypeScript that can run both on a server, 
        embedded device, smartphone or even the browser.
        </aside>
    </section>
    <section data-markdown data-auto-animate data-timing="40">
        {% markdown %}
            ## What is OpenHPS?
            ### An open source hybrid positioning system
            - Support any technology
                - From RF-based to cameras or LIDAR scanners
            - Support any algorithm
                - From basic fingerprinting to GPU intensive computer vision
            - Support any use case
                - Indoor or outdoor positioning or navigation
                - Tabletop positioning
            - Flexibility of outcome
                - Accuracy over battery consumption, reliability, ..
            - Aimed towards ...
                - **Developers**: For creating production-ready positioning systems
                - **Researchers**: For rapid prototyping new algorithms and fusion techniques
        {% endmarkdown %}
        <aside class="notes">
        With OpenHPS we aim to offer a framework for developers and researchers that
        can support a variety of positioning technologies, algorithms and uses cases that are not just
        limited to indoor positioning. We also aim to offer a lot of flexibility in the outcome of the position,
        weither it is a developer wishing to offer the most accurate position, power optimized solution or
        simply a reliable system that can rougly indicate weither or not you are in a specific room.
        </aside>
    </section>
    <section data-markdown data-auto-animate data-timing="25">
        {% markdown %}
            ## Process network design
            
        {% endmarkdown %}

        <img width="90%" src="images/architecture.svg"/>
        <aside class="notes">
        When creating a positioning system, you obtain sensor data from multiple different sensors. You process the data until at some stage in your processing
        you merge the sensor data in order to obtain a more reliable or accurate position that you can show to your user.
        </aside>
    </section>
    <section data-markdown data-auto-animate data-timing="30">
        {% markdown %}
            ## Process network design
            
        {% endmarkdown %}
        <img height="30%" src="images/architecture.svg"/>
        <img height="30%" data-id="architecture-graph" src="images/architecture2.svg"/>
        <aside class="notes">
        OpenHPS focuses on this method of developing a positioning system by allowing the creation of a process network with graph topology. 
        We identify source nodes that provide data, processing nodes that process this data and finally sink nodes that store or display this data. 
        We call this complete process network from source to sink a positioning model.
        </aside>
    </section>
    <section data-markdown data-auto-animate data-timing="35">
        {% markdown %}
            ## Process network design
            
        {% endmarkdown %}
        <img height="20%" src="images/architecture.svg"/>
        <img height="50%" data-id="architecture-graph" src="images/architecture3.svg"/>
        <aside class="notes">
        On top of this, every node in our process network has access to a range of 'Services' that either manage the persistent storage of data that is being handled or
        handle background processing of information that is already pushed through the network. Each service can be added to multiple positioning models, allowing data
        exchange between the calibration part of a process network and the actual online stage used on a production ready system.
        </aside>
    </section>
    <section data-markdown data-auto-animate data-timing="40">
        {% markdown %}
            ## Modularity
        {% endmarkdown %}
        <img width="95%" src="images/stack.svg"/>
        <aside class="notes">
        By creating this process network design with shared services we allow for a very modular ecosystem where individual nodes or services
        can be reused throughout multiple different packages. Our core component offers the main concepts
        needed to design a process network for processing sensor data to an output position while other components
        handle the communication, data storage or add additional algorithms.
        </aside>
    </section>
    <section data-markdown data-auto-animate data-visibility="uncounted" data-timing="0">
        {% markdown %}
            ## Modularity
            **Communication**\
            Socket, MQTT, REST API, ...

            **Data Storage**\
            MongoDB, LocalStorage, RDF, ...

            **Positioning Algorithms**\
            IMU, fingerprinting, OpenVSLAM, ...

            **Abstractions**\
            Geospatial, location based services, geojson ...

            **Misc**\
            React-Native, NativeScript, Sphero ...
        {% endmarkdown %}
        <aside class="notes">
        We already offer many different modules for the more common positioning techinques. In this paper we will use a socket module
        that handles the communication from a mobile application to a server, MongoDB to persist our fingerprinting data and
        fixed beacon locations, the IMU module for pedestrian dead reckoning, symbolic spaces that we will discuss later and finally react-native
        that offers source nodes for sensor data on a smartphone.
        </aside>
    </section>
</section>

<section data-markdown data-auto-animate data-timing="30">
    {% markdown %}
        ## Absolute and Relative positions
        **Absolute**
        - 2D, 3D, Geographical, ...

        **Relative**
        - Distance, angle, velocity, RSSI, ...
        - Relative to another *object*
    {% endmarkdown %}
    <aside class="notes">
    Our framework makes use of the terms absolute and relative position.
    </aside>
</section>

<section>
    <section data-markdown data-auto-animate data-timing="40">
        {% markdown %}
            ## ```DataObject```
            
        {% endmarkdown %}
        <img height="75%" src="images/dataobject.svg"/>
        <aside class="notes">
        We translated these design principles into data objects. A data object is an entity
        that is relevant for the positioning of other data objects. A data object can be considered
        a camera object tracking another object - or it can be a phone that is responsible itself for
        the data collection.

        Each data object can have a hierarchical relation with other objects.
        </aside>
    </section>
    <section data-markdown data-auto-animate data-timing="20">
        {% markdown %}
            ## ```ReferenceSpace```
            *An absolute position can be relative to a reference space.*
        {% endmarkdown %}
        <img width="70%" src="images/referencespace.svg"/>
        <aside class="notes">
        Reference spaces are data objects that represent a transformation of a position. When setting the position or orientation of an object
        you can optionally specify the reference space.

        Similar to data objects, reference spaces can have a hierarchical relation with other objects.
        </aside>
    </section>
    <section data-markdown data-auto-animate data-timing="80">
        {% markdown %}
            ## ```DataFrame```

        {% endmarkdown %}
        <img width="100%" src="images/dataframe.svg"/>
        <aside class="notes">
        Data frames are individual captured moments of sensor data processed in the positioning system. They are created
        by a source object and can contain a set of relevant objects or raw sensor data.

        In this example image we have a video frame that is captured by a camera object. It contains the actual image
        of that frame and a set of objects that were detected inside this frame.

        In the second example we have a simple IMU data frame that captures the currently acceleration of a sensor object.

        And finally, an RF data frame that is created by a receiver and contains relative signal strengths to detected
        access points that are represented as individual data objects.
        </aside>
    </section>
    <section data-markdown data-auto-animate data-visibility="uncounted" data-timing="0">
        {% markdown %}
            ## ```DataFrame```
            ### Pushing data

        {% endmarkdown %}
        <img width="100%" src="images/nodes-push.svg"/>
        <aside class="notes">
        
        </aside>
    </section>
    <section data-markdown data-auto-animate data-visibility="uncounted" data-timing="0">
        {% markdown %}
            ## ```DataFrame```
            ### Pulling data

        {% endmarkdown %}
        <img width="100%" src="images/nodes-pull.svg"/>
        <aside class="notes">
        
        </aside>
    </section>
    <section data-markdown data-auto-animate data-visibility="uncounted" data-timing="0">
        {% markdown %}
            ## ```DataFrame```
            ### Pushing error

        {% endmarkdown %}
        <img width="100%" src="images/nodes-push-error.svg"/>
        <aside class="notes">
        Errors can of course occur in the process network. When this happens, the push is rejected instead of resolved - and the
        last node that performed the failed push will emit an error that will propagate upstream so the upstream nodes
        are aware of the error. It is down to the developer to decide how these errors are handled.
        </aside>
    </section>
</section>

<section>
    <section data-markdown data-auto-animate data-timing="30">
        {% markdown %}
            ## ```SymbolicSpace```
            *An object that semantically defines a space*
        {% endmarkdown %}
        <div class="row">
            <div class="col-7">
                {% markdown %}
                - Spatial hierarchy
                - Connected relation with other spaces
                - Geocoding (space to position, position to space)
                - Can be used as a location
                - GeoJSON import and export
                {% endmarkdown %}
            </div>
            <div class="col-5">
                <img width="100%" src="images/geojson.svg"/>
            <div>
        </div>
        <aside class="notes">
        Symbolic spaces extend the concept 
        </aside>
    </section>
    <section data-markdown data-auto-animate data-visibility="uncounted" data-timing="0">
        {% markdown %}
            ## ```SymbolicSpace```
            
        {% endmarkdown %}
        <div class="row">
            <div class="col-5">
                <img width="100%" style="margin-top: 1em" src="images/symbolicspace.svg"/>
            </div>
            <div class="col-7">
            {% markdown %}
                ```ts
                const building = new Building("PL9")
                    .setBounds({
                        topLeft: new GeographicalPosition(
                                50.8203, 
                                4.3922),
                        width: 46.275, 
                        height: 37.27, 
                        rotation: -34.04
                    });
                
                const floor = new Floor("PL9.3")
                    .setBuilding(building)
                    .setFloorNumber(3);
                
                const office = new Room("PL9.3.58")
                    .setFloor(floor)
                    .setBounds([
                        new Absolute2DPosition(4.75, 31.25),
                        new Absolute2DPosition(8.35, 37.02),
                    ]);
                ```
            {% endmarkdown %}
            </div>
        </div>
        <aside class="notes">

        </aside>
    </section>
</section>

<section>
    <section data-markdown data-auto-animate data-timing="40">
        {% markdown %}
            ## Location-based Service
        {% endmarkdown %}
        <img data-id="lbs-image" src="images/lbs1_h.svg" style="width: 100%;"/>
        <aside class="notes">
        The location based service creates an abstraction of the underlying process network.
        It offers a similar API structure than well known APIs. When a user simply requests the
        current position of a person with a set of options such as the maximum age of the position,
        the LBS will first check if a position is stored in the storage. If not, it will perform a pull
        on the sink of the process network.
        </aside>
    </section>
    <section data-markdown data-auto-animate data-timing="15">
        {% markdown %}
            ## Location-based Service
        {% endmarkdown %}
        <img data-id="lbs-image" src="images/lbs2_h.svg" style="width: 100%;"/>
        <aside class="notes">
        Setting the current position will create an abstraction that uses the current storage
        method to store the new position. This position can then be used by the process network.
        </aside>
    </section>
    <section data-markdown data-auto-animate data-timing="20">
        {% markdown %}
            ## Location-based Service
        {% endmarkdown %}
        <img data-id="lbs-image" src="images/lbs3_h.svg" style="width: 100%;"/>
        <aside class="notes">
        Finally, watching for position changes will listen for new dataframe pushes on the sink
        of the process network. If the positioning system does not update frequently, it will
        trigger a pull every interval.
        </aside>
    </section>
</section>

<section>
    <section data-markdown data-auto-animate data-timing="30">
        {% markdown %}
            ## Demonstration
            ### Positioning model
        {% endmarkdown %}
        <img src="images/model.svg" style="width: 95%;"/>
        <aside class="notes">
        In order to demonstrate that OpenHPS can be used for indoor positioning scenario's we created
        a demonstration using known and common indoor positioning techniques.

        This demonstrator mainly serves as a validation that we can use a process network design for 
        developing a modular indoor positioning system.

        Our demonstration consists out of two applications and a server
        for handling a common implementation with IMU data, WLAN access points and BLE beacons. 
        </aside>
    </section>
    <section data-markdown data-auto-animate data-timing="30">
        {% markdown %}
            ## Positioning model
            ### Offline app
        {% endmarkdown %}
        <div class="row">
            <div class="col-3">
                <img src="images/smartphone-offline.png" width="100%"/>
            </div>
            <div class="col-8">
                <img src="images/model-offline.svg" width="100%"/>
            </div>
        </div>
        <aside class="notes">
        The offline stage application handles the set-up and uses source nodes from our react native component; one that scans for wifi access points,
        one that scans for BLE tags and finally another custom source node for the user location input using
        the smartphone. The user selected location is merged with the scan results of the BLE and WLAN device and
        send to a sink node that acts as a socket interface to the server.
        </aside>
    </section>
       <section data-markdown data-auto-animate data-timing="20">
        {% markdown %}
            ## Positioning model
            ### Server
        {% endmarkdown %}
        <img src="images/model-server-offline.svg" style="width: 90%;"/>
        <aside class="notes">
        On the server this data is received and stored to a shared fingerprinting services
        that extracts the features.
        </aside>
    </section>
    <section data-markdown data-auto-animate data-timing="15">
        {% markdown %}
            ## Positioning model
            ### Online app
        {% endmarkdown %}
        <img src="images/model-online-1.svg" style="width: 90%;"/>
        <aside class="notes">
        The online stage application uses a wifi and ble source similar to the offline stage application. 
        The WLAN and BLE scan results are merged and send to the server using a socket connection
        </aside>
    </section>
    <section data-markdown data-auto-animate data-timing="30">
        {% markdown %}
            ## Positioning model
            ### Server
        {% endmarkdown %}
        <img src="images/model-server-online.svg" style="width: 90%;"/>
        <aside class="notes">
        On the server we use three positioning techniques to obtain a position from this WLAN and BLE data.
        We process the wlan data with a knn fingerprinting algorithm and we perform both ble fingerprinting and
        ble multilateration using the known positions of the beacons in our building. These three outputs are merged and
        send back to the online application.
        </aside>
    </section>
    <section data-markdown data-auto-animate data-timing="60">
        {% markdown %}
            ## Positioning model
            ### Online app
        {% endmarkdown %}
        <img src="images/model-online-2.svg" style="width: 90%;"/>
        <aside class="notes">
        In the online application an IMU source node is used together with pedestrian dead reckoning. The dead reckoning
        is applied on the last calculated position and is used as an intermediate calculate between responses from the server.
        The known velocity of the user is applied to the response from the server in the Velocity processing node named 'a' in order
        to account for the time it took between the wlan or ble scan and the response from the server. Once this processed
        server response is merged with the imu data, it is displayed to the user. An addition feedback loop is created for the predicted
        movement based on the current momentum.
        </aside>
    </section>
    <section data-markdown data-auto-animate data-visibility="uncounted" data-timing="0">
        {% markdown %}
            ## Positioning model
            ### Online app
        {% endmarkdown %}
        <img src="images/pdr.svg" style="height: 70%;"/>
        <aside class="notes">
        
        </aside>
    </section>
    <section data-markdown data-auto-animate data-visibility="uncounted" data-timing="0">
        {% markdown %}
            ## Positioning model
            ### Online app
        {% endmarkdown %}
        {% markdown %}
            ```ts {10-20}
            ModelBuilder.create()
                .addShape(GraphBuilder.create()
                    .from(new IMUSourceNode({
                        source: new DataObject(phoneUID),
                        interval: 20,
                        sensors: [
                            SensorType.ACCELEROMETER, 
                            SensorType.ORIENTATION
                        ]
                    }))
                    .via(new SMAFilterNode(
                        frame => [frame, "acceleration"], 
                        { taps: 10 }
                    ))
                    .via(new GravityProcessingNode({
                        method: GravityProcessingMethod.ABSOLUTE_ORIENTATION
                    }))
                    .via(new PedometerProcessingNode({
                        minConsecutiveSteps: 1, 
                        stepSize: 0.40
                    }))
                    .via(new VelocityProcessingNode())
                    .to("pedometer-output"))
                .build();
            ```
        {% endmarkdown %}
        <aside class="notes">
        
        </aside>
    </section>
</section>

<section>
    <section data-markdown data-auto-animate data-timing="40">
        {% markdown %}
            ## Dataset
        {% endmarkdown %}
        <img data-id="dataset-image" src="images/datapoints.svg" style="width: 75%;"/>
        <aside class="notes">
        For this paper we created a new public dataset recorded using OpenHPS. Our dataset consists of multiple
        training and test data points that are each recorded in four directions. For each orientation we included WLAN
        scan results and BLE scan results from the 11 beacons with known position. We also provide several trajectories
        that also include IMU sensor data.
        </aside>
    </section>
    <section data-markdown data-auto-animate data-visibility="uncounted" data-timing="0">
        {% markdown %}
            ## Dataset
            **Total BLE Beacons**: 11\
            **Total detected WLAN access points**: 220\
            **Total stable WLAN access points**: 199

            ||Training|Test|
            |-|-|-|
            |**Datapoints**|110|30|
            |**Total fingerprints**|440|120|
            |**Duration <small>(per orientation)</small>**|20s|20s|
            |**Avg. WLAN Scans <small>(per fingerprint)</small>**|6|6|
            |**Avg. BLE Advertisements <small>(per fingerprint)</small>**|16|15|
        {% endmarkdown %}
        <aside class="notes">
        We have 110 training datapoints and 30 datapoints, which in total gives us 440 training fingerprints
        and 120 test datapoints. For each fingerprint we waited 20 seconds to collect wifi and ble scan results giving
        us an average of 6 complete wlan scans and 15 individual ble advertisements.
        </aside>
    </section>
</section>

<section>
    <section data-markdown data-auto-animate data-timing="70">
        <style>
        .results table {
            font-size: 75%;
        }

        .results tr > td:last-of-type {
            background-color: #f5f5f5;
        }

        .results tr > th:last-of-type {
            background-color: #f5f5f5;
        }
        </style>
        {% markdown %}
            ## Validation results
            ### Static positioning
        {% endmarkdown %}
        <div class="results">
        {% markdown %}
            ||**WLAN <small>fingerprinting</small>**|**BLE <small>fingerprinting</small>**|**BLE <small>multilateration</small>**|**Fusion**|
            |-|-|-|-|-|
            |*failed points*|0|6|12|0|
            |*average error*|1.23 m|3.23 m|4.92 m|1.37 m|
            |*minimum error*|0.01 m|0.17 m|0.74 m|0.01 m|
            |*maximum error*|4.77 m|15.39 m|19.26 m|9.75 m|
            |*standard deviation*|1.04 m|2.69 m|3.50 m|1.26 m|
            |*hit rate*|95.82 %|80.83 %|52.50 %|96.67 %|
        {% endmarkdown %}
        </div>
        <aside class="notes">
        Using this demonstrator and the dataset that we recorded with the offline part of the demonstrator - we conducted a validation of our
        positioning system.
        </aside>
    </section>
    <section data-markdown data-auto-animate>
        {% markdown %}
            ## Validation results
            ### Trajectories
        {% endmarkdown %}
        <img data-id="trajectories" src="images/trajectories_flipped.svg" style="width: 90%;"/>
        <aside class="notes">
        
        </aside>
    </section>
    <section data-markdown data-auto-animate data-timing="70">
        <style>
        .results.trajectories tr > td:last-of-type {
            background-color: #0034ad;
            color: white;
        }

        .results.trajectories tr > th:last-of-type {
            background-color: #0034ad;
            color: white;
        }
        </style>
        {% markdown %}
            ## Validation results
            ### Trajectories
        {% endmarkdown %}
        <div class="results trajectories">
        {% markdown %}
            ||**WLAN + BLE**|**WLAN + BLE + IMU**|
            |-|-|-|
            |*average error*|3.28 m|1.26 m|
            |*maximum error*|9.60 m|3.10 m|
            |*standard deviation*|3.10 m|0.77 m|
            |*average update frequency*|3.04 s|0.52 m|
        {% endmarkdown %}
        </div>
        <img data-id="trajectories" src="images/trajectories_flipped.svg" style="width: 50%;"/>
        <aside class="notes">

        </aside>
    </section>
</section>

<section>
    <section data-markdown data-auto-animate data-timing="30">
        {% markdown %}
            ## Conclusions & future work
            - OpenHPS: **Open Source** framework for Hybrid Positioning
                - Aimed towards **developers** and **researchers**
            - Public **dataset** with multiple orientations
        {% endmarkdown %}
        <img class="qr-image" src="images/qr-code.svg"/>
        <aside class="notes">
        We have discussed OpenHPS as an open source positioning system framework
        aimed towards developers and researchers.

        We validated this dataset using a new dataset that is available to the public.

        Thank you for your attention
        </aside>
    </section>
</section>